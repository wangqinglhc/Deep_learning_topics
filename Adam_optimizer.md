# Adam optimization algorithm for deep learning
[Link to the paper](https://arxiv.org/abs/1412.6980)

Adam combines the advantages of two other extentions of stochastic gradient descent, which are `Adaptive Gradient Descent` (AdaGrad) and `Root Mean Square Propogation` (RMSProp). It computes individual adaptive learning rates for different parameters from estimates of the first and second moments of the gradients.
