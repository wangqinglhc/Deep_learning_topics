# Adam optimization algorithm for deep learning
[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

Adam combines the advantages of two other extentions of stochastic gradient descent, which are `Adaptive Gradient Descent` (AdaGrad) and `Root Mean Square Propogation` (RMSProp). It computes individual adaptive learning rates for different parameters from estimates of the first and second moments of the gradients.

## Bias-correction approach

## Adam configuration parameters


More papers and blogs for optimization:
[An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747)
[Optimization for deep networks](http://www.cs.cmu.edu/~imisra/data/Optimization_2015_11_11.pdf)(Slides..easier to read:) )
